.. ATTENTION do not edit this file manually.
   It was automatically converted from the corresponding .tex file

.. _section-alwaysincremental:

Always Incremental Backup Scheme
================================

:index:`[TAG=Always Incremental] <single: Always Incremental>`

Always Incremental Backups are available since Bareos :index:`Version >= 16.2.4 <pair: bareos-16.2.4; Always Incremental>`.

Conventional Backup Scheme Drawbacks
------------------------------------

:index:`[TAG=Retention] <single: Retention>`

To better understand the advantages of the Always Incremental Backup scheme, we first analyze the way that the conventional Incremental - Differential - Full Backup Scheme works.

The following figure shows the jobs available for restore over time. Red are full backups, green are differential backups and blue are incremental Backups. When you look for a data at the horizontal axis, you see what backup jobs are available for a restore at this given time.

.. image:: /_static/images/inc-diff-full-jobs_available.*



The next figure shows the amount of data being backed up over the network from that client over time:

.. image:: /_static/images/inc-diff-full-jobdata.*



Depending on the retention periods, old jobs are removed to save space for newer backups:

.. image:: /_static/images/inc-diff-full-jobs_available-zoom.*



The problem with this way of removing jobs is the fact that jobs are removed from the system which existing jobs depend on.

Always Incremental Concept
--------------------------

The Always Incremental Backup Scheme does only incremental backups of clients, which reduces the amount of data transferred over the network to a minimum.

:index:`Always Incremental Backup: Only suitable for file based backups. <triple: Limitation; Always Incremental Backup; Only suitable for file based backups>`
   Always Incremental backups are only suitable for file based backups. Other data can not be combined on the server side (e.g. vmware plugings, NDMP, ...)
   


The Always Incremental Backup Scheme works as follows:

Client Backups are always run as incremental backups. This would usually lead to an unlimited chain of incremental backups that are depend on each other.

To avoid this problem, existing incremental backups older than a configurable age are consolidated into a new backup.

These two steps are then executed every day:

-  Incremental Backup from Client

-  Consolidation of the jobs older than maximum configure age

Deleted files will be in the backup forever, if they are not detected as deleted using **Accurate**:sup:`Dir`:sub:`Job`\  backup.

The Always Incremental Backup Scheme does not provide the option to have other longer retention periods for the backups.

For Longterm Storage of data longer than the Always Incremental Job Retention, there are two options:

-  A copy job can be configured that copies existing full backups into a longterm pool.

-  A virtual Full Job can be configured that creates a virtual full backup into a longterm pool consolidating all existing backups into a new one.

The implementation with copy jobs is easy to implement and automatically copies all jobs that need to be copied in a single configured resource. The disadvantage of the copy job approach is the fact that at a certain point in time, the data that is copied for long term archive is already "always incremental job retention" old, so that the data in the longterm storage is not the current data that is available from the client.

The solution using virtual full jobs to create longterm storage has the disadvantage, that for every backup job the a new longterm job needs to be created.

The big advantage is that the current data will be transferred into the longterm storage.

The way that bareos determines on what base the next incremental job will be done, would choose the longterm storage job to be taken as basis for the next incremental backup which is not what is intended. Therefore, the jobtype of the longterm job is updated to "archive", so that it is not taken as base for then next incrementals and the always incremental job will stand alone.

How to configure in Bareos
--------------------------

Always Incremental Backup Job
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

To configure a job to use Always Incremental Backup Scheme, following configuration is required:

.. code-block:: sh
   :caption: bareos-dir.d/job/example.conf

   Job {
       ...
       Accurate = yes
       Always Incremental = yes
       Always Incremental Job Retention = <timespec>
       Always Incremental Keep Number = <number>
       ...
   }

**Accurate**:sup:`Dir`:sub:`Job`\ = **yes**
   is required to detect deleted files and prevent that they are kept in the consolidated backup jobs.

**Always Incremental**:sup:`Dir`:sub:`Job`\ = **yes**
   enables the Always Incremental feature.

**Always Incremental Job Retention**:sup:`Dir`:sub:`Job`\ 
   set the age where incrementals of this job will be kept, older jobs will be consolidated.

**Always Incremental Keep Number**:sup:`Dir`:sub:`Job`\ 
   sets the number of incrementals that will be kept without regarding the age. This should make sure that a certain history of a job will be kept even if the job is not executed for some time.

**Always Incremental Max Full Age**:sup:`Dir`:sub:`Job`\ 
   is described later, see :ref:`section-AlwaysIncrementalMaxFullAge`.

Consolidate Job
~~~~~~~~~~~~~~~

.. code-block:: sh
   :caption: bareos-dir.d/job/Consolidate.conf

   Job {
       Name = "Consolidate"
       Type = "Consolidate"
       Accurate = "yes"
       JobDefs = "DefaultJob"
   }

\resourceDirectiveValue{Dir}{Job}{Type}{Consolidate}
   configures a job to be a consolidate job. This type have been introduced with the Always Incremental feature. When used, it automatically trigger the consolidation of incremental jobs that need to be consolidated.

**Accurate**:sup:`Dir`:sub:`Job`\ = **yes**
   let the generated virtual backup job keep the accurate information.

**Max Full Consolidations**:sup:`Dir`:sub:`Job`\ 
   is described later, see :ref:`section-MaxFullConsolidations`.

The **Consolidate**:sup:`Dir`:sub:`job`\  job evaluates all jobs configured with **Always Incremental**:sup:`Dir`:sub:`Job`\ = **yes**. When a job is selected for consolidation, all job runs are taken into account, independent of the pool and storage where they are located.

The always incremental jobs need to be executed during the backup window (usually at night), while the consolidation jobs should be scheduled during the daytime when no backups are executed.



.. warning::
   All Bareos job resources have some required directives, e.g. **Client**:sup:`Dir`:sub:`Job`\ .
   Even so, none other than the mentioned directives are evaluated by a \resourceDirectiveValue{Dir}{Job}{Type}{Consolidate},
   they still have to be defined.
   Normally all required directives are already set in \resourceDirectiveValue{Dir}{Job}{Job Defs}{DefaultJob}.
   If not, you have to add them. You can use arbitrary, but valid values.

Storages and Pools
~~~~~~~~~~~~~~~~~~

For the Always Incremental Backup Scheme at least two storages are needed. See :ref:`section-MultipleStorageDevices` how to setup multiple storages.

.. code-block:: sh
   :caption: bareos-dir.d/pool/AI-Incremental.conf

   Pool {
     Name = AI-Incremental
     Pool Type = Backup
     Recycle = yes                       # Bareos can automatically recycle Volumes
     Auto Prune = yes                    # Prune expired volumes
     Volume Retention = 360 days         # How long should jobs be kept?
     Maximum Volume Bytes = 50G          # Limit Volume size to something reasonable
     Label Format = "AI-Incremental-"
     Volume Use Duration = 23h
     Storage = File1
     Next Pool = AI-Consolidated         # consolidated jobs go to this pool
   }

.. code-block:: sh
   :caption: bareos-dir.d/pool/AI-Consolidated.conf

   Pool {
     Name = AI-Consolidated
     Pool Type = Backup
     Recycle = yes                       # Bareos can automatically recycle Volumes
     Auto Prune = yes                    # Prune expired volumes
     Volume Retention = 360 days         # How long should jobs be kept?
     Maximum Volume Bytes = 50G          # Limit Volume size to something reasonable
     Label Format = "AI-Consolidated-"
     Volume Use Duration = 23h
     Storage = File2
     Next Pool = AI-Longterm             # copy jobs write to this pool
   }

.. code-block:: sh
   :caption: bareos-dir.d/pool/AI-Longterm.conf

   Pool {
     Name = AI-Longterm
     Pool Type = Backup
     Recycle = yes                       # Bareos can automatically recycle Volumes
     Auto Prune = yes                    # Prune expired volumes
     Volume Retention = 10 years         # How long should jobs be kept?
     Maximum Volume Bytes = 50G          # Limit Volume size to something reasonable
     Label Format = "AI-Longterm-"
     Volume Use Duration = 23h
     Storage = File1
   }

**AI-Longterm**:sup:`Dir`:sub:`pool`\  is optional and will be explained in :ref:`section-AlwaysIncrementalLongTermStorage`.

How it works
------------

The following configuration extract shows how a client backup is configured for always incremental Backup. The Backup itself is scheduled every night to run as incremental backup, while the consolidation is scheduled to run every day.

.. code-block:: sh
   :caption: bareos-dir.d/job/BackupClient1.conf

   Job {
       Name = "BackupClient1"
       JobDefs = "DefaultJob"

       # Always incremental settings
       AlwaysIncremental = yes
       AlwaysIncrementalJobRetention = 7 days

       Accurate = yes

       Pool = AI-Incremental
       Full Backup Pool = AI-Consolidated
   }

.. code-block:: sh
   :caption: bareos-dir.d/job/Consolidate.conf

   Job {
       Name = "Consolidate"
       Type = "Consolidate"
       Accurate = "yes"
       JobDefs = "DefaultJob"
   }

The following image shows the available backups for each day:

.. image:: /_static/images/always-incremental.*



-  The backup cycle starts with a full backup of the client.

-  Every day a incremental backup is done and is additionally available.

-  When the age of the oldest incremental reaches **Always Incremental Job Retention**:sup:`Dir`:sub:`Job`\ , the consolidation job consolidates the oldest incremental with the full backup before to a new full backup.

This can go on more or less forever and there will be always an incremental history of **Always Incremental Job Retention**:sup:`Dir`:sub:`Job`\ .

The following plot shows what happens if a job is not run for a certain amount of time.

.. image:: /_static/images/always-incremental-with-pause-7days-retention-no-keep.*



As can be seen, the nightly consolidation jobs still go on consolidating until the last incremental is too old and then only one full backup is left. This is usually not what is intended.

For this reason, the directive **Always Incremental Keep Number**:sup:`Dir`:sub:`Job`\  is available which sets the minimum number of incrementals that should be kept even if they are older than **Always Incremental Job Retention**:sup:`Dir`:sub:`Job`\ .

Setting **Always Incremental Keep Number**:sup:`Dir`:sub:`Job`\  to 7 in our case leads to the following result:

.. image:: /_static/images/always-incremental-with-pause-7days-retention-7days-keep.*



**Always Incremental Keep Number**:sup:`Dir`:sub:`Job`\  incrementals are always kept, and when the backup starts again the consolidation of old incrementals starts again.

Enhancements for the Always Incremental Backup Scheme
-----------------------------------------------------

Besides the available backups at each point in time which we have considered until now, the amount of data being moved during the backups is another very important aspect.

We will have a look at this aspect in the following pictures:

The basic always incremental scheme
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The basic always incremental scheme does an incremental backup from the client daily which is relatively small and as such is very good.

During the consolidation, each day the full backup is consolidated with the oldest incremental backup, which means that more or less the full amount of data being stored on the client is moved. Although this consolidation only is performed locally on the storage daemon without client interaction, it is still an enormous amount of data being touched and can take an considerable amount of time.

If all clients use the "always incremental" backup scheme, this means that the complete data being stored in the backup system needs to be moved every day!

This is usually only feasible in relatively small environments.

The following figure shows the Data Volume being moved during the normal always incremental scheme.

-  The red bar shows the amount of the first full backup being copied from the client.

-  The blue bars show the amount of the daily incremental backups. They are so little that the can be barely seen.

-  The green bars show the amount of data being moved every day during the consolidation jobs.

.. image:: /_static/images/always-incremental-jobdata.*



.. _section-AlwaysIncrementalMaxFullAge:

Always Incremental Max Full Age
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

To be able to cope with this problem, the directive **Always Incremental Max Full Age**:sup:`Dir`:sub:`Job`\  was added. When **Always Incremental Max Full Age**:sup:`Dir`:sub:`Job`\  is configured, in daily operation the Full Backup is left untouched while the incrementals are consolidated as usual. Only if the Full Backup is older than **Always Incremental Max Full Age**:sup:`Dir`:sub:`Job`\ , the full backup will also be part of
the consolidation.

Depending on the setting of the **Always Incremental Max Full Age**:sup:`Dir`:sub:`Job`\ , the amount of daily data being moved can be reduced without losing the advantages of the always incremental Backup Scheme.

**Always Incremental Max Full Age**:sup:`Dir`:sub:`Job`\  must be larger than **Always Incremental Job Retention**:sup:`Dir`:sub:`Job`\ .

The resulting interval between full consolidations when running daily backups and daily consolidations is **Always Incremental Max Full Age**:sup:`Dir`:sub:`Job`\  - **Always Incremental Job Retention**:sup:`Dir`:sub:`Job`\ .

\centering

.. figure:: /_static/images/always-incremental-jobdata-AlwaysIncrementalMaxFullAge_21_days.*
   :alt: Data Volume being moved with "Always Incremental Max Full Age"

   Data Volume being moved with "Always Incremental Max Full Age"

\centering

.. figure:: /_static/images/always-incremental-jobs_available-AlwaysIncrementalMaxFullAge_21_days.*
   :alt: Jobs Available with "Always Incremental Max Full Age"

   Jobs Available with "Always Incremental Max Full Age"

.. _section-MaxFullConsolidations:

Max Full Consolidations
~~~~~~~~~~~~~~~~~~~~~~~

When the **Always Incremental Max Full Age**:sup:`Dir`:sub:`Job`\  of many clients is set to the same value, it is probable that all full backups will reach the **Always Incremental Max Full Age**:sup:`Dir`:sub:`Job`\  at once and so consolidation jobs including the full backup will be started for all clients at once. This would again mean that the whole data being stored from all clients will be moved in one day.

The following figure shows the amount of data being copied by the virtual jobs that do the consolidation when having 3 identically configured backup jobs:

.. image:: /_static/images/jobdata_multiple_clients.*



As can be seen, virtual jobs including the full are triggered for all three clients at the same time.

This is of course not desirable so the directive **Max Full Consolidations**:sup:`Dir`:sub:`Job`\  was introduced.

**Max Full Consolidations**:sup:`Dir`:sub:`Job`\  needs to be configured in the \resourceDirectiveValue{Dir}{Job}{Type}{Consolidate} job:

.. code-block:: sh
   :caption: bareos-dir.d/job/Consolidate.conf

   Job {
       Name = "Consolidate"
       Type = "Consolidate"
       Accurate = "yes"
       JobDefs = "DefaultJob"

       Max Full Consolidations = 1
   }

If **Max Full Consolidations**:sup:`Dir`:sub:`Job`\  is configured, the consolidation job will not start more than the specified Consolidations that include the Full Backup.

This leads to a better load balancing of full backup consolidations over different days. The value should configured so that the consolidation jobs are completed before the next normal backup run starts.

The number of always incremental jobs, the interval that the jobs are triggered and the setting of **Always Incremental Max Full Age**:sup:`Dir`:sub:`Job`\  influence the value that makes sense for **Max Full Consolidations**:sup:`Dir`:sub:`Job`\ .

\centering

.. figure:: /_static/images/jobdata_multiple_clients_maxfullconsilidate.*
   :alt: Data Volume being moved with Max Full Consolidations = 1

   Data Volume being moved with Max Full Consolidations = 1

\centering

.. figure:: /_static/images/jobs_available_multiple_clients_maxfullconsolidate.*
   :alt: Jobs Available with Max Full Consolidations = 1

   Jobs Available with Max Full Consolidations = 1

.. _section-AlwaysIncrementalLongTermStorage:

Long Term Storage of Always Incremental Jobs
--------------------------------------------

What is missing in the always incremental backup scheme in comparison to the traditional "Incremental Differential Full" scheme is the option to store a certain job for a longer time.

When using always incremental, the usual maximum age of data stored during the backup cycle is **Always Incremental Job Retention**:sup:`Dir`:sub:`Job`\ .

Usually, it is desired to be able to store a certain backup for a longer time, e.g. monthly a backup should be kept for half a year.

There are two options to achieve this goal.

Copy Jobs
~~~~~~~~~

The configuration of archiving via copy job is simple, just configure a copy job that copies over the latest full backup at that point in time.

As all full backups go into the **AI-Consolidated**:sup:`Dir`:sub:`pool`\ , we just copy all uncopied backups in the **AI-Consolidated**:sup:`Dir`:sub:`pool`\  to a longterm pool:

.. code-block:: sh
   :caption: bareos-dir.d/job/CopyLongtermFull.conf

   Job {
     Name = "CopyLongtermFull"
     Schedule = LongtermFull
     Type = Copy
     Level = Full
     Pool = AI-Consolidated
     Selection Type = PoolUncopiedJobs
     Messages = Standard
   }

As can be seen in the plot, the copy job creates a copy of the current full backup that is available and is already 7 days old.

.. image:: /_static/images/always-incremental-copy-job-archiving.*



The other disadvantage is, that it copies all jobs, not only the virtual full jobs. It also includes the virtual incremental jobs from this pool.

Virtual Full Jobs
~~~~~~~~~~~~~~~~~

The alternative to Copy Jobs is creating a virtual Full Backup Job when the data should be stored in a long-term pool.

.. code-block:: sh
   :caption: bareos-dir.d/job/VirtualLongtermFull.conf

   Job {
     Name = "VirtualLongtermFull"
     Client = bareos-fd
     FileSet = SelfTest
     Schedule = LongtermFull
     Type = Backup
     Level = VirtualFull
     Pool = AI-Consolidated
     Messages = Standard

     Priority = 13                 # run after  Consolidate
     Run Script {
           console = "update jobid=%i jobtype=A"
           Runs When = After
           Runs On Client = No
           Runs On Failure = No
     }
   }

To make sure the longterm \resourceDirectiveValue{Dir}{Job}{Level}{VirtualFull} is not taken as base for the next incrementals, the job type of the copied job is set to \resourceDirectiveValue{Dir}{Job}{Type}{Archive} with the **Run Script**:sup:`Dir`:sub:`Job`\ .

As can be seen on the plot, the \resourceDirectiveValue{Dir}{Job}{Level}{VirtualFull} archives the current data, i.e. it consolidates the full and all incrementals that are currently available.

.. image:: /_static/images/always-incremental-virtualfull-job-archiving.*



How to manually transfer data/volumes
=====================================

The always incremental backup scheme minimizes the amount of data that needs to be transferred over the wire.

This makes it possible to backup big filesystems over small bandwidths.

The only challenge is to do the first full backup.

The easiest way to transfer the data is to copy it to a portable data medium (or even directly store it on there) and import the data into the local bareos catalog as if it was backed up from the original client.

This can be done in two ways

#. Install a storage daemon in the remote location that needs to be backed up and connect it to the main director. This makes it easy to make a local backup in the remote location and then transfer the volumes to the local storage. For this option the communication between the local director and the remote storage daemon needs to be possible.

.. image:: /_static/images/ai-transfer-first-backup2.*



#. Install a director and a storage daemon in the remote location. This option means that the backup is done completely independent from the local director and only the volume is then transferred and needs to be imported afterwards.

.. image:: /_static/images/ai-transfer-first-backup3.*




Import Data from a Remote Storage Daemon
----------------------------------------

First setup client, fileset, job and schedule as needed for a always incremental backup of the remote client.

Run the first backup but make sure that you choose the remote storage to be used.

.. code-block:: sh
   :caption: run

   *run job=BackupClient-remote level=Full storage=File-remote

Transport the volumes that were used for that backup over to the local storage daemon and make them available to the local storage daemon. This can be either by putting the tapes into the local changer or by storing the file volumes into the local file volume directory.

If copying a volume to the local storage directory make sure that the file rights are correct.

Now tell the director that the volume now belongs to the local storage daemon.

List volumes shows that the volumes used still belong to the remote storage:

.. code-block:: sh
   :caption: list volumes

   *<input>list volumes</input>
   .....
   Pool: Full
   +---------+------------+-----------+---------+----------+----------+--------------+---------+------+-----------+-----------+---------------------+-------------+
   | MediaId | VolumeName | VolStatus | Enabled | VolBytes | VolFiles | VolRetention | Recycle | Slot | InChanger | MediaType | LastWritten         | Storage     |
   +---------+------------+-----------+---------+----------+----------+--------------+---------+------+-----------+-----------+---------------------+-------------+
   | 1       | Full-0001  | Append    | 1       | 38600329 | 0        | 31536000     | 1       | 0    | 0         | File      | 2016-07-28 14:00:47 | File-remote |
   +---------+------------+-----------+---------+----------+----------+--------------+---------+------+-----------+-----------+---------------------+-------------+

Use :strong:`update volume` to set the right storage and check with list volumes that it worked:

.. code-block:: sh
   :caption: update volume

   *<input>update volume=Full-0001 storage=File</input>
   *<input>list volumes</input>
   ...
   Pool: Full
   +---------+------------+-----------+---------+----------+----------+--------------+---------+------+-----------+-----------+---------------------+---------+
   | MediaId | VolumeName | VolStatus | Enabled | VolBytes | VolFiles | VolRetention | Recycle | Slot | InChanger | MediaType | LastWritten         | Storage |
   +---------+------------+-----------+---------+----------+----------+--------------+---------+------+-----------+-----------+---------------------+---------+
   | 1       | Full-0001  | Append    | 1       | 38600329 | 0        | 31536000     | 1       | 0    | 0         | File      | 2016-07-28 14:00:47 | File    |
   +---------+------------+-----------+---------+----------+----------+--------------+---------+------+-----------+-----------+---------------------+---------+

Now the remote storage daemon can be disabled as it is not needed anymore.

The next incremental run will take the previously taken full backup as reference.

Import Data from a Independent Remote Full Bareos Installation
--------------------------------------------------------------

If a network connection between the local director and the remote storage daemon is not possible, it is also an option to setup a fully functional Bareos installation remotely and then to import the created volumes. Of course the network connection between the |bareosDir| and the |bareosFd| is needed in any case to make the incremental backups possible.

-  Configure the connection from local |bareosDir| to remote |bareosFd|, give the remote client the same name as it was when the data was backed up.

-  Add the Fileset created on remote machine to local machine.

-  Configure the Job that should backup the remote client with the fileset.

-  Run :strong:`estimate listing` on the remote backup job.

-  Run :strong:`list filesets` to make sure the fileset was added to the catalog.

Then we need to create a backup on the remote machine onto a portable disk which we can then import into our local installation.

On remote machine:

-  Install full Bareos server on remote server (sd, fd, dir). Using the Sqlite backend is sufficient.

-  Add the client to the remote backup server.

-  Add fileset which the client will be backed up.

-  Add Pool with name **transfer**:sup:`Dir`:sub:`pool`\  where the data will be written to.

-  create job that will backup the remote client with the remote fileset into the new pool

-  Do the local backup using the just created Pool and Filesets.

Transport the newly created volume over to the director machine (e.g. via external harddrive) and store the file where the device stores its files (e.g. /var/lib/bareos/storage)

Shutdown Director on local director machine.

Import data form volume via :command:`bscan`, you need to set which database backend is used: :command:`bscan -B sqlite3 FileStorage -V Transfer-0001 -s -S`

If the import was successfully completed, test if an incremental job really only backs up the minimum amount of data.













